{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1264c22-e628-453e-9ce2-44d262aafcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import BertForSequenceClassification,AutoTokenizer\n",
    "from torch.utils.data import DataLoader,Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2cb41237-4fe2-44d9-be0b-5759870184b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializig device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40692bb4-dcde-446f-aa8a-c771fea21c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=load_dataset(\"imdb\")[0]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fa2339-abe2-45c4-98be-b86e0f74a055",
   "metadata": {},
   "source": [
    "### Custom Dataset Class\n",
    "\n",
    "We define a custom PyTorch Dataset to wrap our tokenized IMDB data.  \n",
    "This makes it compatible with DataLoader for batching and shuffling during training.\n",
    "\n",
    "Each item returned includes:\n",
    "- input_ids\n",
    "- attention_mask\n",
    "- labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "771fa04c-e2e0-410e-ab35-0187444d0d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom PyTorch Dataset to work with tokenized data\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,tokens):\n",
    "        self.input_id=tokens[\"input_ids\"]\n",
    "        self.attention_mask=tokens[\"attention_mask\"]\n",
    "        self.labels=tokens[\"labels\"]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return{\"input_ids\":self.input_id[index],\n",
    "              \"attention_mask\":self.attention_mask[index],\n",
    "              \"labels\":self.labels[index]}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8480af77-513c-4932-ba4a-fac4db63f6d9",
   "metadata": {},
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained(\"bert-base-uncased\",num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "86454e31-859b-4c28-a7fa-cb14f5f41563",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#A function to tokenize prepare dataLoader\n",
    "def load_and_tokenize_data(tokenizer, data,mode =\"train\", max_len=512, sample_size=15,shuffle = True):\n",
    "    samples = data[mode][0:sample_size][\"text\"] #slicing few samples only bcos of no gpu\n",
    "    tokens=tokenizer(samples,max_length=max_len,truncation=True,padding=\"max_length\",return_tensors=\"pt\")\n",
    "     # Adding labels\n",
    "    tokens['labels'] = data[mode][0:sample_size][\"label\"]\n",
    "    # Wrap tokens with customDataset and return dataLoader\n",
    "    dataloader_samples = DataLoader(CustomDataset(tokens),shuffle =shuffle)\n",
    "    return tokens, dataloader_samples"
   ]
  },
  {
   "cell_type": "raw",
   "id": "665defe7-5d89-4897-bc61-60f8aac5cd6d",
   "metadata": {},
   "source": [
    "\n",
    "model=BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "76e5aa07-f597-49b3-ad28-7c58551f53d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "def train_model_func(model, train_loader, optimizer, device=\"cpu\", epochs=3):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    epoch = epochs\n",
    "    for e in range(epoch):\n",
    "        total_loss = 0\n",
    "        for batch in train_loader :\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            output = model(input_ids,attention_mask,labels =labels)\n",
    "            loss = output.loss \n",
    "            total_loss = total_loss + loss.item()\n",
    "            loss.backward() #here gradients will be calc using backpropagation\n",
    "            optimizer.step() #Update weights\n",
    "            optimizer.zero_grad() #Reset gradients\n",
    "        print(f\"epoch {e +1}-------{total_loss}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8b037c8a-707f-4a40-b7b6-54ac916d1a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to evaluate the model\n",
    "\n",
    "def eval_func(model,test_loader):\n",
    "    model.eval()\n",
    "    actual_label = 0\n",
    "    pred_labels = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids=batch[\"input_ids\"].to(device)\n",
    "            attention_mask=batch[\"attention_mask\"].to(device)\n",
    "            labels=batch[\"labels\"].to(device)\n",
    "            output = model(input_ids,attention_mask,labels =labels)\n",
    "            logit = output.logits \n",
    "            pred = torch.argmax(logit,dim = 1) \n",
    "            actual_label = actual_label + (pred == labels).sum().item()\n",
    "            pred_labels = pred_labels + labels.size(0)\n",
    "        accuracy = actual_label / pred_labels\n",
    "        print(\"accuracy_score\",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "44557a53-2b9a-4982-8962-9fdba58582d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to make prediction on new text\n",
    "def prediction_func(model,text,tokenizer,max_len=512):\n",
    "    sentiments={0:\"Negative\",1:\"Positive\"}\n",
    "    tokens=tokenizer(text,max_length=max_len,truncation=True,padding=\"max_length\",return_tensors=\"pt\")\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    attention_mask=tokens[\"attention_mask\"].to(device)\n",
    "    input_ids=tokens[\"input_ids\"].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output=model(attention_mask=attention_mask,input_ids=input_ids)\n",
    "        logits=output.logits\n",
    "        pred=torch.argmax(logits,dim=1).item()\n",
    "        \n",
    "        return sentiments[pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c54a4bed-3a9d-4b3f-bec8-e3a86f5dfba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initializing BertFor Classification model here\n",
    "model=BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d16856fa-cc6a-4956-83dc-3802ec6c035c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Tokenizer\n",
    "tokenizer=AutoTokenizer.from_pretrained(\"bert-base-uncased\",num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9a5e6837-898d-4e0f-8c32-c37d88839ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting a optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "95c9e05b-2133-4162-a28f-777946ec035d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1-------5.485954730771482\n",
      "epoch 2-------0.3543743977788836\n"
     ]
    }
   ],
   "source": [
    "# Now we are using our previous func to test a new review\n",
    "\n",
    "tokens,train_loader = load_and_tokenize_data(tokenizer, data, mode=\"train\", sample_size=50)\n",
    "tokens,test_loader = load_and_tokenize_data(tokenizer, data, mode=\"test\", sample_size=30, shuffle=False)\n",
    "\n",
    "\n",
    "# train the model\n",
    "train_model_func(model, train_loader, optimizer, device=device, epochs=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0b049896-e939-47e4-be74-5d226afc18a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score 1.0\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "eval_func(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "44168abc-c6c3-426a-8112-7e4a27873446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: Negative\n"
     ]
    }
   ],
   "source": [
    "# test prediction\n",
    "sample_text = \"this movie was absolutely fantastic, i loved every part of it\"\n",
    "print(\"prediction:\", prediction_func(model, sample_text, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30907627-72e9-4253-840b-2b83740ab9fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
